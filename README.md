# ğŸ“˜ Daily Notes â€“ AI & RAG Learning

*A continuously updated repository of daily notes, concepts, and explanations related to AI, RAG (Retrieval-Augmented Generation), LangChain, Vector Databases, Embeddings, and more.*

---

## ğŸš€ Overview

This repository contains my **daily learning notes**, explanations, and practical insights about modern AI developmentâ€”especially around **RAG pipelines**, **document ingestion**, **LLM context**, **similarity search**, and **LangChain document structure**.

The notes are updated daily to track continuous learning and progress.

---

## ğŸ“… Daily Topics Covered So Far

### ğŸ”µ Retrieval-Augmented Generation (RAG)

RAG = *LLM + external knowledge source*
The model retrieves relevant information from a vector database and uses it to generate accurate answers.

### ğŸ”¹ Before RAG

Models relied only on training data â†’ outdated & hallucinations.

### ğŸ”¹ After RAG

Models use **external documents** (PDF, Docs, HTML, SQL, etc.) through a retrieval pipeline.

---

## ğŸ§© Key Concepts Learned

### âœ… **1. Context**

The relevant information retrieved from your vector DB that the LLM uses to answer a query.
Example:
User: *What is the leave policy?*
Retrieved text: *Employees get 24 annual leavesâ€¦*

---

### âœ… **2. Similarity Search (Top-K)**

* Converts query â†’ vector
* Compares it with stored vectors
* Returns top-K most relevant chunks
* Uses **cosine similarity**

  * +1 â†’ perfect match
  * 0 â†’ not related
  * â€“1 â†’ opposite

---

### âœ… **3. Query vs Prompt**

| Query               | Prompt                                |
| ------------------- | ------------------------------------- |
| Userâ€™s raw question | Query + Context + System Instructions |
| â€œLeave policy?â€     | Full formatted input sent to LLM      |

---

## ğŸ—ï¸ Pipelines

### ğŸ”µ **Data Ingestion Pipeline** (Offline)

Runs **before** user asks anything.

Steps:

1. Load documents
2. Parse
3. Chunk
4. Embed
5. Store in vector DB

Goal â†’ Prepare the knowledge base.

---

### ğŸ”´ **Retrieval Pipeline** (Online)

Runs **when user asks a question**.

Steps:

1. Query â†’ embedding
2. Similarity search
3. Retrieve chunks
4. Build prompt
5. Generate answer

Goal â†’ Use the knowledge base.

---

## ğŸ“¦ Document Structure in LangChain

Every document =

```python
Document(
    page_content="Actual text",
    metadata={"source": "file.pdf", "page": 2}
)
```

Components:

* **page_content** â†’ text
* **metadata** â†’ source, page, section, etc.

---

## ğŸ§ª LangChain Example Code

### **Chunking Example**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=40, chunk_overlap=0)
chunks = splitter.split_documents(docs)
```

### **Embedding Example**

```python
from langchain_openai import OpenAIEmbeddings
embedder = OpenAIEmbeddings()
vectors = embedder.embed_documents([c.page_content for c in chunks])
```

### **Vector Store**

```python
from langchain.vectorstores import Chroma
store = Chroma.from_documents(chunks, embedder)
```

---

## ğŸ§  Tokens & LLM Context

âœ” Token = small piece of text
âœ” 100,000 tokens â‰ˆ 150 pages of text
âœ” Important for RAG because **context window is limited**

---

## ğŸ“ˆ What This Repository Tracks

* Daily concepts
* Notes from courses
* Code snippets
* Architecture diagrams
* RAG workflows
* LangChain experiments
* Vector DB insights
* And moreâ€¦
---

## âœ¨ Purpose

To serve as:
âœ” My personal learning log
âœ” A reference for AI/RAG concepts
âœ” A structured repository anyone can learn from

---
âœ… convert notes into markdown files
Just tell me!
